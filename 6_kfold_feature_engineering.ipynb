{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **YAP 101 Homework-6**  \n",
    "Due Date: 23 March 2020 23:59\n",
    "\n",
    "Send your files to m.torusdag@etu.edu.tr\n",
    "\n",
    "The subject of your email should be \"YAP101 HW6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will use the data nba_logreg.csv filw which is uploaded on Piazza. Each row represents an NBA player with their stats such as PTS (points per game),  BLK (blocks), STL (steals) etc. The last column shows whether the player's career length is more than 5 years (1) or not (0). Your task is to build a knn model that predicts whether a player's career length is more than 5 years or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first\n",
    "from datascience import * \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (5 Points)**. Load the data. We want to use the first 80% of the data for training and the last 20% for testing. Split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write your answer here\n",
    "data=Table.read_table('nba_logreg.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_units(x):\n",
    "    return (x - np.mean(x))/np.std(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Name</th> <th>GP</th> <th>MIN</th> <th>PTS</th> <th>FGM</th> <th>FGA</th> <th>FG%</th> <th>3P Made</th> <th>3PA</th> <th>3P%</th> <th>FTM</th> <th>FTA</th> <th>FT%</th> <th>OREB</th> <th>DREB</th> <th>REB</th> <th>AST</th> <th>STL</th> <th>BLK</th> <th>TOV</th> <th>TARGET_5Yrs</th> <th>GP-su</th> <th>MIN-su</th> <th>PTS-su</th> <th>FGM-su</th> <th>FGA-su</th> <th>FG%-su</th> <th>3P Made-su</th> <th>3PA-su</th> <th>3P%-su</th> <th>FTM-su</th> <th>FTA-su</th> <th>FT%-su</th> <th>OREB-su</th> <th>DREB-su</th> <th>REB-su</th> <th>AST-su</th> <th>STL-su</th> <th>BLK-su</th> <th>TOV-su</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>Brandon Ingram </td> <td>36  </td> <td>27.4</td> <td>7.4 </td> <td>2.6 </td> <td>7.6 </td> <td>34.7</td> <td>0.5    </td> <td>2.1 </td> <td>25  </td> <td>1.6 </td> <td>2.3 </td> <td>69.9</td> <td>0.7 </td> <td>3.4 </td> <td>4.1 </td> <td>1.9 </td> <td>0.4 </td> <td>0.4 </td> <td>1.3 </td> <td>0          </td> <td>-1.4009  </td> <td>1.17707  </td> <td>0.137401  </td> <td>-0.017294 </td> <td>0.477347  </td> <td>-1.54341 </td> <td>0.658041  </td> <td>1.24435  </td> <td>nan   </td> <td>0.306333  </td> <td>0.361484  </td> <td>-0.037855</td> <td>-0.39829  </td> <td>1.01085  </td> <td>0.517997 </td> <td>0.23764  </td> <td>-0.533458 </td> <td>0.0732542</td> <td>0.147338 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Andrew Harrison</td> <td>35  </td> <td>26.9</td> <td>7.2 </td> <td>2   </td> <td>6.7 </td> <td>29.6</td> <td>0.7    </td> <td>2.8 </td> <td>23.5</td> <td>2.6 </td> <td>3.4 </td> <td>76.5</td> <td>0.5 </td> <td>2   </td> <td>2.4 </td> <td>3.7 </td> <td>1.1 </td> <td>0.5 </td> <td>1.6 </td> <td>0          </td> <td>-1.45828 </td> <td>1.11686  </td> <td>0.0914864 </td> <td>-0.373816 </td> <td>0.226801  </td> <td>-2.37465 </td> <td>1.17949   </td> <td>1.90383  </td> <td>nan   </td> <td>1.31963   </td> <td>1.19325   </td> <td>0.586286 </td> <td>-0.655746 </td> <td>-0.018938</td> <td>-0.308447</td> <td>1.46161  </td> <td>1.1755    </td> <td>0.306415 </td> <td>0.562695 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>JaKarr Sampson </td> <td>74  </td> <td>15.3</td> <td>5.2 </td> <td>2   </td> <td>4.7 </td> <td>42.2</td> <td>0.4    </td> <td>1.7 </td> <td>24.4</td> <td>0.9 </td> <td>1.3 </td> <td>67  </td> <td>0.5 </td> <td>1.7 </td> <td>2.2 </td> <td>1   </td> <td>0.5 </td> <td>0.3 </td> <td>1   </td> <td>0          </td> <td>0.779563 </td> <td>-0.279912</td> <td>-0.367659 </td> <td>-0.373816 </td> <td>-0.329969 </td> <td>-0.320991</td> <td>0.397315  </td> <td>0.867511 </td> <td>nan   </td> <td>-0.402975 </td> <td>-0.394665 </td> <td>-0.312099</td> <td>-0.655746 </td> <td>-0.239607</td> <td>-0.405676</td> <td>-0.374347</td> <td>-0.289321 </td> <td>-0.159906</td> <td>-0.268019</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Malik Sealy    </td> <td>58  </td> <td>11.6</td> <td>5.7 </td> <td>2.3 </td> <td>5.5 </td> <td>42.6</td> <td>0.1    </td> <td>0.5 </td> <td>22.6</td> <td>0.9 </td> <td>1.3 </td> <td>68.9</td> <td>1   </td> <td>0.9 </td> <td>1.9 </td> <td>0.8 </td> <td>0.6 </td> <td>0.1 </td> <td>1   </td> <td>1          </td> <td>-0.138527</td> <td>-0.725434</td> <td>-0.252873 </td> <td>-0.195555 </td> <td>-0.107261 </td> <td>-0.255795</td> <td>-0.384863 </td> <td>-0.263016</td> <td>nan   </td> <td>-0.402975 </td> <td>-0.394665 </td> <td>-0.132422</td> <td>-0.0121043</td> <td>-0.828059</td> <td>-0.551519</td> <td>-0.510344</td> <td>-0.0451836</td> <td>-0.626228</td> <td>-0.268019</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Matt Geiger    </td> <td>48  </td> <td>11.5</td> <td>4.5 </td> <td>1.6 </td> <td>3   </td> <td>52.4</td> <td>0      </td> <td>0.1 </td> <td>0   </td> <td>1.3 </td> <td>1.9 </td> <td>67.4</td> <td>1   </td> <td>1.5 </td> <td>2.5 </td> <td>0.3 </td> <td>0.3 </td> <td>0.4 </td> <td>0.8 </td> <td>1          </td> <td>-0.712333</td> <td>-0.737475</td> <td>-0.52836  </td> <td>-0.611497 </td> <td>-0.803224 </td> <td>1.3415   </td> <td>-0.645589 </td> <td>-0.639859</td> <td>nan   </td> <td>0.00234419</td> <td>0.0590248 </td> <td>-0.274272</td> <td>-0.0121043</td> <td>-0.38672 </td> <td>-0.259833</td> <td>-0.850337</td> <td>-0.777595 </td> <td>0.0732542</td> <td>-0.544923</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Tony Bennett   </td> <td>75  </td> <td>11.4</td> <td>3.7 </td> <td>1.5 </td> <td>3.5 </td> <td>42.3</td> <td>0.3    </td> <td>1.1 </td> <td>32.5</td> <td>0.4 </td> <td>0.5 </td> <td>73.2</td> <td>0.2 </td> <td>0.7 </td> <td>0.8 </td> <td>1.8 </td> <td>0.4 </td> <td>0   </td> <td>0.7 </td> <td>0          </td> <td>0.836943 </td> <td>-0.749516</td> <td>-0.712018 </td> <td>-0.670917 </td> <td>-0.664031 </td> <td>-0.304692</td> <td>0.136589  </td> <td>0.302247 </td> <td>nan   </td> <td>-0.909623 </td> <td>-0.999584 </td> <td>0.274216 </td> <td>-1.04193  </td> <td>-0.975172</td> <td>-1.08628 </td> <td>0.169641 </td> <td>-0.533458 </td> <td>-0.859388</td> <td>-0.683375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Don MacLean    </td> <td>62  </td> <td>10.9</td> <td>6.6 </td> <td>2.5 </td> <td>5.8 </td> <td>43.5</td> <td>0      </td> <td>0.1 </td> <td>50  </td> <td>1.5 </td> <td>1.8 </td> <td>81.1</td> <td>0.5 </td> <td>1.4 </td> <td>2   </td> <td>0.6 </td> <td>0.2 </td> <td>0.1 </td> <td>0.7 </td> <td>1          </td> <td>0.0909954</td> <td>-0.809722</td> <td>-0.0462572</td> <td>-0.0767143</td> <td>-0.0237458</td> <td>-0.109105</td> <td>-0.645589 </td> <td>-0.639859</td> <td>nan   </td> <td>0.205004  </td> <td>-0.0165901</td> <td>1.02129  </td> <td>-0.655746 </td> <td>-0.460277</td> <td>-0.502905</td> <td>-0.646341</td> <td>-1.02173  </td> <td>-0.626228</td> <td>-0.683375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Tracy Murray   </td> <td>48  </td> <td>10.3</td> <td>5.7 </td> <td>2.3 </td> <td>5.4 </td> <td>41.5</td> <td>0.4    </td> <td>1.5 </td> <td>30  </td> <td>0.7 </td> <td>0.8 </td> <td>87.5</td> <td>0.8 </td> <td>0.9 </td> <td>1.7 </td> <td>0.2 </td> <td>0.2 </td> <td>0.1 </td> <td>0.7 </td> <td>1          </td> <td>-0.712333</td> <td>-0.881968</td> <td>-0.252873 </td> <td>-0.195555 </td> <td>-0.1351   </td> <td>-0.435083</td> <td>0.397315  </td> <td>0.67909  </td> <td>nan   </td> <td>-0.605634 </td> <td>-0.772739 </td> <td>1.62652  </td> <td>-0.269561 </td> <td>-0.828059</td> <td>-0.648748</td> <td>-0.918335</td> <td>-1.02173  </td> <td>-0.626228</td> <td>-0.683375</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Duane Cooper   </td> <td>65  </td> <td>9.9 </td> <td>2.4 </td> <td>1   </td> <td>2.4 </td> <td>39.2</td> <td>0.1    </td> <td>0.5 </td> <td>23.3</td> <td>0.4 </td> <td>0.5 </td> <td>71.4</td> <td>0.2 </td> <td>0.6 </td> <td>0.8 </td> <td>2.3 </td> <td>0.3 </td> <td>0   </td> <td>1.1 </td> <td>0          </td> <td>0.263137 </td> <td>-0.930133</td> <td>-1.01046  </td> <td>-0.968019 </td> <td>-0.970255 </td> <td>-0.809957</td> <td>-0.384863 </td> <td>-0.263016</td> <td>nan   </td> <td>-0.909623 </td> <td>-0.999584 </td> <td>0.103995 </td> <td>-1.04193  </td> <td>-1.04873 </td> <td>-1.08628 </td> <td>0.509634 </td> <td>-0.777595 </td> <td>-0.859388</td> <td>-0.129566</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Dave Johnson   </td> <td>42  </td> <td>8.5 </td> <td>3.7 </td> <td>1.4 </td> <td>3.5 </td> <td>38.3</td> <td>0.1    </td> <td>0.3 </td> <td>21.4</td> <td>1   </td> <td>1.4 </td> <td>67.8</td> <td>0.4 </td> <td>0.7 </td> <td>1.1 </td> <td>0.3 </td> <td>0.2 </td> <td>0   </td> <td>0.7 </td> <td>0          </td> <td>-1.05662 </td> <td>-1.09871 </td> <td>-0.712018 </td> <td>-0.730337 </td> <td>-0.664031 </td> <td>-0.956647</td> <td>-0.384863 </td> <td>-0.451438</td> <td>nan   </td> <td>-0.301645 </td> <td>-0.31905  </td> <td>-0.236445</td> <td>-0.784475 </td> <td>-0.975172</td> <td>-0.940434</td> <td>-0.850337</td> <td>-1.02173  </td> <td>-0.859388</td> <td>-0.683375</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (1330 rows omitted)</p>"
      ],
      "text/plain": [
       "Name            | GP   | MIN  | PTS  | FGM  | FGA  | FG%  | 3P Made | 3PA  | 3P%  | FTM  | FTA  | FT%  | OREB | DREB | REB  | AST  | STL  | BLK  | TOV  | TARGET_5Yrs | GP-su     | MIN-su    | PTS-su     | FGM-su     | FGA-su     | FG%-su    | 3P Made-su | 3PA-su    | 3P%-su | FTM-su     | FTA-su     | FT%-su    | OREB-su    | DREB-su   | REB-su    | AST-su    | STL-su     | BLK-su    | TOV-su\n",
       "Brandon Ingram  | 36   | 27.4 | 7.4  | 2.6  | 7.6  | 34.7 | 0.5     | 2.1  | 25   | 1.6  | 2.3  | 69.9 | 0.7  | 3.4  | 4.1  | 1.9  | 0.4  | 0.4  | 1.3  | 0           | -1.4009   | 1.17707   | 0.137401   | -0.017294  | 0.477347   | -1.54341  | 0.658041   | 1.24435   | nan    | 0.306333   | 0.361484   | -0.037855 | -0.39829   | 1.01085   | 0.517997  | 0.23764   | -0.533458  | 0.0732542 | 0.147338\n",
       "Andrew Harrison | 35   | 26.9 | 7.2  | 2    | 6.7  | 29.6 | 0.7     | 2.8  | 23.5 | 2.6  | 3.4  | 76.5 | 0.5  | 2    | 2.4  | 3.7  | 1.1  | 0.5  | 1.6  | 0           | -1.45828  | 1.11686   | 0.0914864  | -0.373816  | 0.226801   | -2.37465  | 1.17949    | 1.90383   | nan    | 1.31963    | 1.19325    | 0.586286  | -0.655746  | -0.018938 | -0.308447 | 1.46161   | 1.1755     | 0.306415  | 0.562695\n",
       "JaKarr Sampson  | 74   | 15.3 | 5.2  | 2    | 4.7  | 42.2 | 0.4     | 1.7  | 24.4 | 0.9  | 1.3  | 67   | 0.5  | 1.7  | 2.2  | 1    | 0.5  | 0.3  | 1    | 0           | 0.779563  | -0.279912 | -0.367659  | -0.373816  | -0.329969  | -0.320991 | 0.397315   | 0.867511  | nan    | -0.402975  | -0.394665  | -0.312099 | -0.655746  | -0.239607 | -0.405676 | -0.374347 | -0.289321  | -0.159906 | -0.268019\n",
       "Malik Sealy     | 58   | 11.6 | 5.7  | 2.3  | 5.5  | 42.6 | 0.1     | 0.5  | 22.6 | 0.9  | 1.3  | 68.9 | 1    | 0.9  | 1.9  | 0.8  | 0.6  | 0.1  | 1    | 1           | -0.138527 | -0.725434 | -0.252873  | -0.195555  | -0.107261  | -0.255795 | -0.384863  | -0.263016 | nan    | -0.402975  | -0.394665  | -0.132422 | -0.0121043 | -0.828059 | -0.551519 | -0.510344 | -0.0451836 | -0.626228 | -0.268019\n",
       "Matt Geiger     | 48   | 11.5 | 4.5  | 1.6  | 3    | 52.4 | 0       | 0.1  | 0    | 1.3  | 1.9  | 67.4 | 1    | 1.5  | 2.5  | 0.3  | 0.3  | 0.4  | 0.8  | 1           | -0.712333 | -0.737475 | -0.52836   | -0.611497  | -0.803224  | 1.3415    | -0.645589  | -0.639859 | nan    | 0.00234419 | 0.0590248  | -0.274272 | -0.0121043 | -0.38672  | -0.259833 | -0.850337 | -0.777595  | 0.0732542 | -0.544923\n",
       "Tony Bennett    | 75   | 11.4 | 3.7  | 1.5  | 3.5  | 42.3 | 0.3     | 1.1  | 32.5 | 0.4  | 0.5  | 73.2 | 0.2  | 0.7  | 0.8  | 1.8  | 0.4  | 0    | 0.7  | 0           | 0.836943  | -0.749516 | -0.712018  | -0.670917  | -0.664031  | -0.304692 | 0.136589   | 0.302247  | nan    | -0.909623  | -0.999584  | 0.274216  | -1.04193   | -0.975172 | -1.08628  | 0.169641  | -0.533458  | -0.859388 | -0.683375\n",
       "Don MacLean     | 62   | 10.9 | 6.6  | 2.5  | 5.8  | 43.5 | 0       | 0.1  | 50   | 1.5  | 1.8  | 81.1 | 0.5  | 1.4  | 2    | 0.6  | 0.2  | 0.1  | 0.7  | 1           | 0.0909954 | -0.809722 | -0.0462572 | -0.0767143 | -0.0237458 | -0.109105 | -0.645589  | -0.639859 | nan    | 0.205004   | -0.0165901 | 1.02129   | -0.655746  | -0.460277 | -0.502905 | -0.646341 | -1.02173   | -0.626228 | -0.683375\n",
       "Tracy Murray    | 48   | 10.3 | 5.7  | 2.3  | 5.4  | 41.5 | 0.4     | 1.5  | 30   | 0.7  | 0.8  | 87.5 | 0.8  | 0.9  | 1.7  | 0.2  | 0.2  | 0.1  | 0.7  | 1           | -0.712333 | -0.881968 | -0.252873  | -0.195555  | -0.1351    | -0.435083 | 0.397315   | 0.67909   | nan    | -0.605634  | -0.772739  | 1.62652   | -0.269561  | -0.828059 | -0.648748 | -0.918335 | -1.02173   | -0.626228 | -0.683375\n",
       "Duane Cooper    | 65   | 9.9  | 2.4  | 1    | 2.4  | 39.2 | 0.1     | 0.5  | 23.3 | 0.4  | 0.5  | 71.4 | 0.2  | 0.6  | 0.8  | 2.3  | 0.3  | 0    | 1.1  | 0           | 0.263137  | -0.930133 | -1.01046   | -0.968019  | -0.970255  | -0.809957 | -0.384863  | -0.263016 | nan    | -0.909623  | -0.999584  | 0.103995  | -1.04193   | -1.04873  | -1.08628  | 0.509634  | -0.777595  | -0.859388 | -0.129566\n",
       "Dave Johnson    | 42   | 8.5  | 3.7  | 1.4  | 3.5  | 38.3 | 0.1     | 0.3  | 21.4 | 1    | 1.4  | 67.8 | 0.4  | 0.7  | 1.1  | 0.3  | 0.2  | 0    | 0.7  | 0           | -1.05662  | -1.09871  | -0.712018  | -0.730337  | -0.664031  | -0.956647 | -0.384863  | -0.451438 | nan    | -0.301645  | -0.31905   | -0.236445 | -0.784475  | -0.975172 | -0.940434 | -0.850337 | -1.02173   | -0.859388 | -0.683375\n",
       "... (1330 rows omitted)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalabels=data.labels\n",
    "for i in np.arange(1,len(datalabels)-1): #for dropping the target column\n",
    "    data=data.with_columns(data.labels[i]+'-su',standard_units(data.column(i)))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GP-su', 'MIN-su', 'PTS-su', 'FGM-su', 'FGA-su', 'FG%-su',\n",
       "       '3P Made-su', '3PA-su', '3P%-su', 'FTM-su', 'FTA-su', 'FT%-su',\n",
       "       'OREB-su', 'DREB-su', 'REB-su', 'AST-su', 'STL-su', 'BLK-su',\n",
       "       'TOV-su'], dtype='<U32')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes=make_array()\n",
    "for i in np.arange(1,len(datalabels)-1):\n",
    "    attributes=np.append(attributes,data.labels[i]+'-su')\n",
    "attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sd=data.select(attributes)\n",
    "data_sd=data_sd.with_column('TARGET_5Yrs',data.column('TARGET_5Yrs')).drop('3P%-su')\n",
    "train,test=data_sd.split(int(data.num_rows*0.8))\n",
    "#'3P%-su' columnd dropped because there ara nan values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>GP-su</th> <th>MIN-su</th> <th>PTS-su</th> <th>FGM-su</th> <th>FGA-su</th> <th>FG%-su</th> <th>3P Made-su</th> <th>3PA-su</th> <th>FTM-su</th> <th>FTA-su</th> <th>FT%-su</th> <th>OREB-su</th> <th>DREB-su</th> <th>REB-su</th> <th>AST-su</th> <th>STL-su</th> <th>BLK-su</th> <th>TOV-su</th> <th>TARGET_5Yrs</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>-1.4009  </td> <td>1.17707  </td> <td>0.137401  </td> <td>-0.017294 </td> <td>0.477347  </td> <td>-1.54341 </td> <td>0.658041  </td> <td>1.24435  </td> <td>0.306333  </td> <td>0.361484  </td> <td>-0.037855</td> <td>-0.39829  </td> <td>1.01085  </td> <td>0.517997 </td> <td>0.23764  </td> <td>-0.533458 </td> <td>0.0732542</td> <td>0.147338 </td> <td>0          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-1.45828 </td> <td>1.11686  </td> <td>0.0914864 </td> <td>-0.373816 </td> <td>0.226801  </td> <td>-2.37465 </td> <td>1.17949   </td> <td>1.90383  </td> <td>1.31963   </td> <td>1.19325   </td> <td>0.586286 </td> <td>-0.655746 </td> <td>-0.018938</td> <td>-0.308447</td> <td>1.46161  </td> <td>1.1755    </td> <td>0.306415 </td> <td>0.562695 </td> <td>0          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.779563 </td> <td>-0.279912</td> <td>-0.367659 </td> <td>-0.373816 </td> <td>-0.329969 </td> <td>-0.320991</td> <td>0.397315  </td> <td>0.867511 </td> <td>-0.402975 </td> <td>-0.394665 </td> <td>-0.312099</td> <td>-0.655746 </td> <td>-0.239607</td> <td>-0.405676</td> <td>-0.374347</td> <td>-0.289321 </td> <td>-0.159906</td> <td>-0.268019</td> <td>0          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.138527</td> <td>-0.725434</td> <td>-0.252873 </td> <td>-0.195555 </td> <td>-0.107261 </td> <td>-0.255795</td> <td>-0.384863 </td> <td>-0.263016</td> <td>-0.402975 </td> <td>-0.394665 </td> <td>-0.132422</td> <td>-0.0121043</td> <td>-0.828059</td> <td>-0.551519</td> <td>-0.510344</td> <td>-0.0451836</td> <td>-0.626228</td> <td>-0.268019</td> <td>1          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.712333</td> <td>-0.737475</td> <td>-0.52836  </td> <td>-0.611497 </td> <td>-0.803224 </td> <td>1.3415   </td> <td>-0.645589 </td> <td>-0.639859</td> <td>0.00234419</td> <td>0.0590248 </td> <td>-0.274272</td> <td>-0.0121043</td> <td>-0.38672 </td> <td>-0.259833</td> <td>-0.850337</td> <td>-0.777595 </td> <td>0.0732542</td> <td>-0.544923</td> <td>1          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.836943 </td> <td>-0.749516</td> <td>-0.712018 </td> <td>-0.670917 </td> <td>-0.664031 </td> <td>-0.304692</td> <td>0.136589  </td> <td>0.302247 </td> <td>-0.909623 </td> <td>-0.999584 </td> <td>0.274216 </td> <td>-1.04193  </td> <td>-0.975172</td> <td>-1.08628 </td> <td>0.169641 </td> <td>-0.533458 </td> <td>-0.859388</td> <td>-0.683375</td> <td>0          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0909954</td> <td>-0.809722</td> <td>-0.0462572</td> <td>-0.0767143</td> <td>-0.0237458</td> <td>-0.109105</td> <td>-0.645589 </td> <td>-0.639859</td> <td>0.205004  </td> <td>-0.0165901</td> <td>1.02129  </td> <td>-0.655746 </td> <td>-0.460277</td> <td>-0.502905</td> <td>-0.646341</td> <td>-1.02173  </td> <td>-0.626228</td> <td>-0.683375</td> <td>1          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.712333</td> <td>-0.881968</td> <td>-0.252873 </td> <td>-0.195555 </td> <td>-0.1351   </td> <td>-0.435083</td> <td>0.397315  </td> <td>0.67909  </td> <td>-0.605634 </td> <td>-0.772739 </td> <td>1.62652  </td> <td>-0.269561 </td> <td>-0.828059</td> <td>-0.648748</td> <td>-0.918335</td> <td>-1.02173  </td> <td>-0.626228</td> <td>-0.683375</td> <td>1          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.263137 </td> <td>-0.930133</td> <td>-1.01046  </td> <td>-0.968019 </td> <td>-0.970255 </td> <td>-0.809957</td> <td>-0.384863 </td> <td>-0.263016</td> <td>-0.909623 </td> <td>-0.999584 </td> <td>0.103995 </td> <td>-1.04193  </td> <td>-1.04873 </td> <td>-1.08628 </td> <td>0.509634 </td> <td>-0.777595 </td> <td>-0.859388</td> <td>-0.129566</td> <td>0          </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-1.05662 </td> <td>-1.09871 </td> <td>-0.712018 </td> <td>-0.730337 </td> <td>-0.664031 </td> <td>-0.956647</td> <td>-0.384863 </td> <td>-0.451438</td> <td>-0.301645 </td> <td>-0.31905  </td> <td>-0.236445</td> <td>-0.784475 </td> <td>-0.975172</td> <td>-0.940434</td> <td>-0.850337</td> <td>-1.02173  </td> <td>-0.859388</td> <td>-0.683375</td> <td>0          </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (1330 rows omitted)</p>"
      ],
      "text/plain": [
       "GP-su     | MIN-su    | PTS-su     | FGM-su     | FGA-su     | FG%-su    | 3P Made-su | 3PA-su    | FTM-su     | FTA-su     | FT%-su    | OREB-su    | DREB-su   | REB-su    | AST-su    | STL-su     | BLK-su    | TOV-su    | TARGET_5Yrs\n",
       "-1.4009   | 1.17707   | 0.137401   | -0.017294  | 0.477347   | -1.54341  | 0.658041   | 1.24435   | 0.306333   | 0.361484   | -0.037855 | -0.39829   | 1.01085   | 0.517997  | 0.23764   | -0.533458  | 0.0732542 | 0.147338  | 0\n",
       "-1.45828  | 1.11686   | 0.0914864  | -0.373816  | 0.226801   | -2.37465  | 1.17949    | 1.90383   | 1.31963    | 1.19325    | 0.586286  | -0.655746  | -0.018938 | -0.308447 | 1.46161   | 1.1755     | 0.306415  | 0.562695  | 0\n",
       "0.779563  | -0.279912 | -0.367659  | -0.373816  | -0.329969  | -0.320991 | 0.397315   | 0.867511  | -0.402975  | -0.394665  | -0.312099 | -0.655746  | -0.239607 | -0.405676 | -0.374347 | -0.289321  | -0.159906 | -0.268019 | 0\n",
       "-0.138527 | -0.725434 | -0.252873  | -0.195555  | -0.107261  | -0.255795 | -0.384863  | -0.263016 | -0.402975  | -0.394665  | -0.132422 | -0.0121043 | -0.828059 | -0.551519 | -0.510344 | -0.0451836 | -0.626228 | -0.268019 | 1\n",
       "-0.712333 | -0.737475 | -0.52836   | -0.611497  | -0.803224  | 1.3415    | -0.645589  | -0.639859 | 0.00234419 | 0.0590248  | -0.274272 | -0.0121043 | -0.38672  | -0.259833 | -0.850337 | -0.777595  | 0.0732542 | -0.544923 | 1\n",
       "0.836943  | -0.749516 | -0.712018  | -0.670917  | -0.664031  | -0.304692 | 0.136589   | 0.302247  | -0.909623  | -0.999584  | 0.274216  | -1.04193   | -0.975172 | -1.08628  | 0.169641  | -0.533458  | -0.859388 | -0.683375 | 0\n",
       "0.0909954 | -0.809722 | -0.0462572 | -0.0767143 | -0.0237458 | -0.109105 | -0.645589  | -0.639859 | 0.205004   | -0.0165901 | 1.02129   | -0.655746  | -0.460277 | -0.502905 | -0.646341 | -1.02173   | -0.626228 | -0.683375 | 1\n",
       "-0.712333 | -0.881968 | -0.252873  | -0.195555  | -0.1351    | -0.435083 | 0.397315   | 0.67909   | -0.605634  | -0.772739  | 1.62652   | -0.269561  | -0.828059 | -0.648748 | -0.918335 | -1.02173   | -0.626228 | -0.683375 | 1\n",
       "0.263137  | -0.930133 | -1.01046   | -0.968019  | -0.970255  | -0.809957 | -0.384863  | -0.263016 | -0.909623  | -0.999584  | 0.103995  | -1.04193   | -1.04873  | -1.08628  | 0.509634  | -0.777595  | -0.859388 | -0.129566 | 0\n",
       "-1.05662  | -1.09871  | -0.712018  | -0.730337  | -0.664031  | -0.956647 | -0.384863  | -0.451438 | -0.301645  | -0.31905   | -0.236445 | -0.784475  | -0.975172 | -0.940434 | -0.850337 | -1.02173   | -0.859388 | -0.683375 | 0\n",
       "... (1330 rows omitted)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sd  #data with standard unit values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (30 Points)** Build a knn classifier which uses all numericals columns (except Target_5Yrs) as features to predict Target_5Yrs using the training data. Set the k parameter of knn to 3. Calculate your classifier's accuracy on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_distances(training, new_point):\n",
    "    \"\"\"Returns an array of distances\n",
    "    between each point in the training set\n",
    "    and the new point (which is a row of attributes)\"\"\"\n",
    "    attributes = training.drop('TARGET_5Yrs')\n",
    "    def distance_from_point(row):\n",
    "        return euclidian_distance(np.array(new_point), np.array(row))\n",
    "    return attributes.apply(distance_from_point)\n",
    "\n",
    "def table_with_distances(training, new_point):\n",
    "    \"\"\"Augments the training table \n",
    "    with a column of distances from new_point\"\"\"\n",
    "    return training.with_column('Distance', all_distances(training, new_point))\n",
    "\n",
    "def closest(training, new_point, k):\n",
    "    \"\"\"Returns a table of the k rows of the augmented table\n",
    "    corresponding to the k smallest distances\"\"\"\n",
    "    with_dists = table_with_distances(training, new_point)\n",
    "    sorted_by_distance = with_dists.sort('Distance')\n",
    "    topk = sorted_by_distance.take(np.arange(k))\n",
    "    return topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority(topkclasses):\n",
    "    ones = topkclasses.where('TARGET_5Yrs', are.equal_to(1)).num_rows\n",
    "    zeros = topkclasses.where('TARGET_5Yrs', are.equal_to(0)).num_rows\n",
    "    if ones > zeros:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def classify(training, new_point, k):\n",
    "    closestk = closest(training, new_point, k)\n",
    "    topkclasses = closestk.select('TARGET_5Yrs')\n",
    "    return majority(topkclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zero(array):\n",
    "    \"\"\"Counts the number of 0's in an array\"\"\"\n",
    "    return len(array) - np.count_nonzero(array)\n",
    "\n",
    "def count_equal(array1, array2):\n",
    "    \"\"\"Takes two numerical arrays of equal length\n",
    "    and counts the indices where the two are equal\"\"\"\n",
    "    return count_zero(array1 - array2)\n",
    "\n",
    "def evaluate_accuracy(training, test, k):\n",
    "    test_attributes = test.drop('TARGET_5Yrs')\n",
    "    def classify_testrow(row):\n",
    "        return classify(training, row, k)\n",
    "    c = test_attributes.apply(classify_testrow)\n",
    "    return count_equal(c, test.column('TARGET_5Yrs')) / test.num_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6194029850746269"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(train, test, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (10 Points) Which k value is the best?** For the same model, try k = 1,2,3,4, and 5 and show which k value yields the best accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>k</th> <th>accuracy</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>0.593284</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2   </td> <td>0.526119</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3   </td> <td>0.619403</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4   </td> <td>0.589552</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5   </td> <td>0.641791</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "k    | accuracy\n",
       "1    | 0.593284\n",
       "2    | 0.526119\n",
       "3    | 0.619403\n",
       "4    | 0.589552\n",
       "5    | 0.641791"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy=make_array()\n",
    "for i in np.arange(1,6):\n",
    "    accuracy=np.append(accuracy,evaluate_accuracy(train, test, i))\n",
    "\n",
    "accuracy_table=Table().with_columns('k',np.arange(1,6),'accuracy',accuracy)\n",
    "accuracy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_value=(accuracy_table.sort('accuracy',descending=True).take(0)).column(0)[0]\n",
    "k_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (30 Points)Feature Engineering** We want to know which feautures are the most useful for this task. Set the value k to the value which gives the best performance in Question 3. Then for each feature you used previously, build a seperate knn model using the training data and all features except the corresponding feature, and calculate the performance on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>feature</th> <th>accuracy</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0      </td> <td>0.641791</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1      </td> <td>0.630597</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2      </td> <td>0.660448</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3      </td> <td>0.645522</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4      </td> <td>0.63806 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5      </td> <td>0.671642</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6      </td> <td>0.652985</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7      </td> <td>0.660448</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8      </td> <td>0.645522</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9      </td> <td>0.641791</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>10     </td> <td>0.626866</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>11     </td> <td>0.652985</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>12     </td> <td>0.615672</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13     </td> <td>0.634328</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14     </td> <td>0.63806 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15     </td> <td>0.645522</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>16     </td> <td>0.649254</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>17     </td> <td>0.649254</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc=make_array()\n",
    "for i in np.arange(len(data_sd.labels)-1):\n",
    "    acc_value=evaluate_accuracy(train.drop(i), test.drop(i), k_value)\n",
    "    acc=np.append(acc,acc_value)\n",
    "\n",
    "accuracy_table_feature=Table().with_columns('feature',np.arange(len(data_sd.labels)-1),'accuracy',acc)\n",
    "accuracy_table_feature.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max performance of features\n",
    "dropped_feature_id=accuracy_table_feature.sort('accuracy',descending=True).take(0).column(0)[0]\n",
    "dropped_feature_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5(25 Points)K-fold**. In Q4, you have applied a leave-one-out method such that you exclude one feature and build the model with the rest. So if the performance of the model increases when we exclude a feature (compared to the model we use all features), that feature might not be a suitable feature for this task. So, exclude all features (if any) that yield higher performance when excluded than the model using all features. Then build the model with the remaining features. However, this time you will apply 5-fold cross validation. For cross-validation, do not shuffle the data. Report the accuracy for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(data,k1,k2):\n",
    "    index = np.arange(data.num_rows)\n",
    "    #shuffled_index = index.sample(with_replacement=False)\n",
    "    test_size = int(data.num_rows/k1)\n",
    "    print (test_size)\n",
    "    total_accuracy = 0\n",
    "    for i in np.arange(k1):\n",
    "        test_index = index[i*test_size:(i+1)*test_size]\n",
    "        train_index = np.setdiff1d(index, test_index)\n",
    "        test = data.take(test_index)\n",
    "        train = data.take(train_index)\n",
    "        accuracy = evaluate_accuracy(train, test, k2)\n",
    "        total_accuracy = total_accuracy + accuracy\n",
    "        print (accuracy)\n",
    "    return total_accuracy/k1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "0.6082089552238806\n",
      "0.6529850746268657\n",
      "0.6455223880597015\n",
      "0.6604477611940298\n",
      "0.664179104477612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.646268656716418"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold(data_sd.drop(dropped_feature_id),5,k_value)  # with dropped column,the model has the best score,therefore it is dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
