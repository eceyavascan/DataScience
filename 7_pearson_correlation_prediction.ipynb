{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **YAP 101 Homework-7**  \n",
    "Due Date: 30 March 2020 23:59\n",
    "\n",
    "Send your files to m.torusdag@etu.edu.tr\n",
    "\n",
    "The subject of your email should be \"YAP101 HW7\"\n",
    "\n",
    "Total score is 105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you will use the  forestfires.csv file which is uploaded on Piazza. Each row represents a forest fire in Portugal with several attributes and the area of the fire. Your task is to build a regression model that predicts the fire area. The columns in the file are as follows. \n",
    "\n",
    "    X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "    Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "    month - month of the year: 'jan' to 'dec'\n",
    "    day - day of the week: 'mon' to 'sun'\n",
    "    FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "    DMC - DMC index from the FWI system: 1.1 to 291.3\n",
    "    DC - DC index from the FWI system: 7.9 to 860.6\n",
    "    ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "    temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "    RH - relative humidity in %: 15.0 to 100\n",
    "    wind - wind speed in km/h: 0.40 to 9.40\n",
    "    rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "    area - the burned area of the forest (in ha): 0.00 to 1090.84 (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first\n",
    "from datascience import * \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (5 Points)**. Load the data. We want to use the first 80% of the data for training and the last 20% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Table.read_table('forestfires.csv')\n",
    "month_class=data.select('month').group('month').column('month')\n",
    "day_class=data.select('day').group('day').column('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>X</th> <th>Y</th> <th>month</th> <th>day</th> <th>FFMC</th> <th>DMC</th> <th>DC</th> <th>ISI</th> <th>temp</th> <th>RH</th> <th>wind</th> <th>rain</th> <th>area</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>5   </td> <td>mar  </td> <td>fri </td> <td>86.2</td> <td>26.2 </td> <td>94.3 </td> <td>5.1 </td> <td>8.2 </td> <td>51  </td> <td>6.7 </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>4   </td> <td>oct  </td> <td>tue </td> <td>90.6</td> <td>35.4 </td> <td>669.1</td> <td>6.7 </td> <td>18  </td> <td>33  </td> <td>0.9 </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>4   </td> <td>oct  </td> <td>sat </td> <td>90.6</td> <td>43.7 </td> <td>686.9</td> <td>6.7 </td> <td>14.6</td> <td>33  </td> <td>1.3 </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>6   </td> <td>mar  </td> <td>fri </td> <td>91.7</td> <td>33.3 </td> <td>77.5 </td> <td>9   </td> <td>8.3 </td> <td>97  </td> <td>4   </td> <td>0.2 </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>6   </td> <td>mar  </td> <td>sun </td> <td>89.3</td> <td>51.3 </td> <td>102.2</td> <td>9.6 </td> <td>11.4</td> <td>99  </td> <td>1.8 </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>6   </td> <td>aug  </td> <td>sun </td> <td>92.3</td> <td>85.3 </td> <td>488  </td> <td>14.7</td> <td>22.2</td> <td>29  </td> <td>5.4 </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>6   </td> <td>aug  </td> <td>mon </td> <td>92.3</td> <td>88.9 </td> <td>495.6</td> <td>8.5 </td> <td>24.1</td> <td>27  </td> <td>3.1 </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>6   </td> <td>aug  </td> <td>mon </td> <td>91.5</td> <td>145.4</td> <td>608.2</td> <td>10.7</td> <td>8   </td> <td>86  </td> <td>2.2 </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8   </td> <td>6   </td> <td>sep  </td> <td>tue </td> <td>91  </td> <td>129.5</td> <td>692.6</td> <td>7   </td> <td>13.1</td> <td>63  </td> <td>5.4 </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7   </td> <td>5   </td> <td>sep  </td> <td>sat </td> <td>92.5</td> <td>88   </td> <td>698.6</td> <td>7.1 </td> <td>22.8</td> <td>40  </td> <td>4   </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (507 rows omitted)</p>"
      ],
      "text/plain": [
       "X    | Y    | month | day  | FFMC | DMC   | DC    | ISI  | temp | RH   | wind | rain | area\n",
       "7    | 5    | mar   | fri  | 86.2 | 26.2  | 94.3  | 5.1  | 8.2  | 51   | 6.7  | 0    | 0\n",
       "7    | 4    | oct   | tue  | 90.6 | 35.4  | 669.1 | 6.7  | 18   | 33   | 0.9  | 0    | 0\n",
       "7    | 4    | oct   | sat  | 90.6 | 43.7  | 686.9 | 6.7  | 14.6 | 33   | 1.3  | 0    | 0\n",
       "8    | 6    | mar   | fri  | 91.7 | 33.3  | 77.5  | 9    | 8.3  | 97   | 4    | 0.2  | 0\n",
       "8    | 6    | mar   | sun  | 89.3 | 51.3  | 102.2 | 9.6  | 11.4 | 99   | 1.8  | 0    | 0\n",
       "8    | 6    | aug   | sun  | 92.3 | 85.3  | 488   | 14.7 | 22.2 | 29   | 5.4  | 0    | 0\n",
       "8    | 6    | aug   | mon  | 92.3 | 88.9  | 495.6 | 8.5  | 24.1 | 27   | 3.1  | 0    | 0\n",
       "8    | 6    | aug   | mon  | 91.5 | 145.4 | 608.2 | 10.7 | 8    | 86   | 2.2  | 0    | 0\n",
       "8    | 6    | sep   | tue  | 91   | 129.5 | 692.6 | 7    | 13.1 | 63   | 5.4  | 0    | 0\n",
       "7    | 5    | sep   | sat  | 92.5 | 88    | 698.6 | 7.1  | 22.8 | 40   | 4    | 0    | 0\n",
       "... (507 rows omitted)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>X</th> <th>Y</th> <th>FFMC</th> <th>DMC</th> <th>DC</th> <th>ISI</th> <th>temp</th> <th>RH</th> <th>wind</th> <th>rain</th> <th>area</th> <th>apr</th> <th>aug</th> <th>dec</th> <th>feb</th> <th>jan</th> <th>jul</th> <th>jun</th> <th>mar</th> <th>may</th> <th>nov</th> <th>oct</th> <th>sep</th> <th>fri</th> <th>mon</th> <th>sat</th> <th>sun</th> <th>thu</th> <th>tue</th> <th>wed</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>4   </td> <td>4   </td> <td>83  </td> <td>23.3</td> <td>85.3 </td> <td>2.3 </td> <td>16.7</td> <td>20  </td> <td>3.1 </td> <td>0   </td> <td>0   </td> <td>1   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>1   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6   </td> <td>5   </td> <td>63.5</td> <td>70.8</td> <td>665.3</td> <td>0.8 </td> <td>17  </td> <td>72  </td> <td>6.7 </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>1   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>1   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1   </td> <td>2   </td> <td>90.1</td> <td>108 </td> <td>529.8</td> <td>12.5</td> <td>14.7</td> <td>66  </td> <td>2.7 </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>1   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>1   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td> <td>0   </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (514 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data2 = data\n",
    "for i in np.arange(0,len(month_class)):\n",
    "    labels = np.zeros(len(month_class))\n",
    "    labels[i] = 1\n",
    "    tmp_table = Table().with_columns('class',month_class,month_class[i],labels)\n",
    "    data2 = data2.join('month',tmp_table,'class')\n",
    "\n",
    "for i in np.arange(0,len(day_class)):\n",
    "    labels = np.zeros(len(day_class))\n",
    "    labels[i] = 1\n",
    "    tmp_table = Table().with_columns('class',day_class,day_class[i],labels)\n",
    "    data2 = data2.join('day',tmp_table,'class')\n",
    "    \n",
    "data2 = data2.drop('month','day')\n",
    "data2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=data2.split(int(data.num_rows*0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (15 Points)** Build a linear regression model to predict area of forest fires using all features. Print the weigths of each feature and bias value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(weights, bias, x):\n",
    "    return sum(weights*x) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_areas = train.column('area')  #array\n",
    "train_features = train.drop('area')  #table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(weights, bias, features, prices):\n",
    "    errors = []\n",
    "    for i in np.arange(len(prices)):\n",
    "        predicted = predict_price(weights,bias, np.array(features.row(i)))\n",
    "        actual = prices.item(i)\n",
    "        errors.append((predicted - actual) ** 2)\n",
    "    return np.mean(errors) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_train(weights):\n",
    "    return rmse(weights[0:len(weights)-1],weights[len(weights)-1], train_features, train_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best slopes for the training set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>X</th> <th>Y</th> <th>FFMC</th> <th>DMC</th> <th>DC</th> <th>ISI</th> <th>temp</th> <th>RH</th> <th>wind</th> <th>rain</th> <th>apr</th> <th>aug</th> <th>dec</th> <th>feb</th> <th>jan</th> <th>jul</th> <th>jun</th> <th>mar</th> <th>may</th> <th>nov</th> <th>oct</th> <th>sep</th> <th>fri</th> <th>mon</th> <th>sat</th> <th>sun</th> <th>thu</th> <th>tue</th> <th>wed</th> <th>bias</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2.75341</td> <td>0.0435671</td> <td>0.21177</td> <td>0.229855</td> <td>-0.146933</td> <td>-0.472837</td> <td>1.10813</td> <td>-0.235157</td> <td>1.7195</td> <td>-2.45874</td> <td>-31.5072</td> <td>27.8959</td> <td>24.3832</td> <td>-17.1299</td> <td>-26.9405</td> <td>13.6388</td> <td>-19.8975</td> <td>-32.2866</td> <td>-13.0747</td> <td>-26.7657</td> <td>51.288</td> <td>58.0695</td> <td>-6.34157</td> <td>1.07384</td> <td>16.0532</td> <td>-5.82395</td> <td>5.49572</td> <td>-1.20193</td> <td>-6.56449</td> <td>-3.32168</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of all training examples using the best slopes: 67.82695528679277\n"
     ]
    }
   ],
   "source": [
    "initial_weights = np.ones( len(train_features.row(0) )+1)\n",
    "\n",
    "#minimize fonksyonu ile elde edilen weightler\n",
    "learned_weights = minimize(rmse_train, start=initial_weights, smooth=True, array=True)\n",
    "\n",
    "print('The best slopes for the training set:')\n",
    "features_names = np.append(train_features.labels,'bias')\n",
    "Table(features_names).with_row(list(learned_weights)).show()\n",
    "\n",
    "print('RMSE of all training examples using the best slopes:', rmse_train(learned_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (15 Points)** We want to know which feautures are correlated each other. Calculate pearson correlation between each feature pair. Print the results in a matrix format (the diagonals should be 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix = [[pearsonr(train_features.column(i),train_features.column(j))[0] for i in np.arange(len(train_features.labels))] for j in np.arange(len(train_features.labels))]\n",
    "matrix_table=Table(train_features.labels)\n",
    "\n",
    "for i in np.arange(len(train_features.labels)):\n",
    "    matrix_table=matrix_table.with_row(list(Matrix[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>X</th> <th>Y</th> <th>FFMC</th> <th>DMC</th> <th>DC</th> <th>ISI</th> <th>temp</th> <th>RH</th> <th>wind</th> <th>rain</th> <th>apr</th> <th>aug</th> <th>dec</th> <th>feb</th> <th>jan</th> <th>jul</th> <th>jun</th> <th>mar</th> <th>may</th> <th>nov</th> <th>oct</th> <th>sep</th> <th>fri</th> <th>mon</th> <th>sat</th> <th>sun</th> <th>thu</th> <th>tue</th> <th>wed</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1         </td> <td>0.530113   </td> <td>-0.0357917 </td> <td>-0.0103306 </td> <td>-0.0772409</td> <td>-0.0241371 </td> <td>-0.0669356</td> <td>0.0993985  </td> <td>0.0403303  </td> <td>0.0809352  </td> <td>0.0780156  </td> <td>-0.0350897</td> <td>0.00854172 </td> <td>0.0363699  </td> <td>-0.0538299 </td> <td>0.0514758  </td> <td>0.108693   </td> <td>0.00545336</td> <td>0.0157337  </td> <td>0.0327592  </td> <td>0.0901702  </td> <td>-0.103218 </td> <td>-0.0505909 </td> <td>0.036501   </td> <td>0.0169414 </td> <td>-0.0600415 </td> <td>-0.0117728</td> <td>0.0148926  </td> <td>0.0697063 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.530113  </td> <td>1          </td> <td>-0.0260394 </td> <td>0.0486021  </td> <td>-0.0758083</td> <td>-0.0159348 </td> <td>-0.0167555</td> <td>0.0508049  </td> <td>0.0191882  </td> <td>0.0419085  </td> <td>0.0146118  </td> <td>0.0262867 </td> <td>0.0739206  </td> <td>-0.0511023 </td> <td>-0.0104705 </td> <td>0.0607627  </td> <td>0.0970676  </td> <td>0.0481744 </td> <td>-0.0148255 </td> <td>-0.0520503 </td> <td>-0.0351559 </td> <td>-0.110605 </td> <td>-0.0761121 </td> <td>0.0173001  </td> <td>0.01034   </td> <td>-0.028835  </td> <td>0.0193415 </td> <td>-0.00362522</td> <td>0.0806525 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0357917</td> <td>-0.0260394 </td> <td>1          </td> <td>0.396743   </td> <td>0.360603  </td> <td>0.64644    </td> <td>0.456054  </td> <td>-0.252286  </td> <td>-0.104632  </td> <td>0.0713877  </td> <td>-0.163959  </td> <td>0.270168  </td> <td>-0.166887  </td> <td>-0.335407  </td> <td>-0.0886557 </td> <td>0.0162477  </td> <td>-0.0760959 </td> <td>-0.0995258</td> <td>-0.0492187 </td> <td>-0.115347  </td> <td>-0.00249694</td> <td>0.0638747 </td> <td>0.00252989 </td> <td>-0.107021  </td> <td>-0.0516326</td> <td>-0.0120804 </td> <td>0.0588916 </td> <td>0.00471689 </td> <td>0.126117  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0103306</td> <td>0.0486021  </td> <td>0.396743   </td> <td>1          </td> <td>0.695748  </td> <td>0.334986   </td> <td>0.465616  </td> <td>0.121964   </td> <td>-0.150409  </td> <td>0.0806306  </td> <td>-0.209151  </td> <td>0.501085  </td> <td>-0.18765   </td> <td>-0.296502  </td> <td>-0.0827246 </td> <td>0.0186327  </td> <td>-0.0557266 </td> <td>-0.424646 </td> <td>-0.0923705 </td> <td>-0.0832568 </td> <td>-0.178424  </td> <td>0.10278   </td> <td>-0.0466028 </td> <td>-0.147485  </td> <td>-0.0115224</td> <td>0.0759467  </td> <td>0.119571  </td> <td>-0.00239231</td> <td>0.0234951 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0772409</td> <td>-0.0758083 </td> <td>0.360603   </td> <td>0.695748   </td> <td>1         </td> <td>0.310284   </td> <td>0.49979   </td> <td>-0.0134172 </td> <td>-0.250767  </td> <td>0.0403531  </td> <td>-0.280427  </td> <td>0.298465  </td> <td>-0.110946  </td> <td>-0.364331  </td> <td>-0.106655  </td> <td>-0.0929711 </td> <td>-0.169593  </td> <td>-0.669985 </td> <td>-0.1273    </td> <td>-0.087337  </td> <td>0.0913372  </td> <td>0.51514   </td> <td>-0.0419475 </td> <td>-0.0694226 </td> <td>-0.0340239</td> <td>0.0398968  </td> <td>0.0765573 </td> <td>0.0145859  </td> <td>0.0271583 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0241371</td> <td>-0.0159348 </td> <td>0.64644    </td> <td>0.334986   </td> <td>0.310284  </td> <td>1          </td> <td>0.445814  </td> <td>-0.112952  </td> <td>0.0722512  </td> <td>0.0828252  </td> <td>-0.154585  </td> <td>0.406754  </td> <td>-0.204334  </td> <td>-0.271685  </td> <td>-0.0734151 </td> <td>0.030646   </td> <td>-0.00203346</td> <td>-0.166378 </td> <td>-0.0752487 </td> <td>-0.0948784 </td> <td>-0.0586931 </td> <td>-0.0634771</td> <td>0.0358406  </td> <td>-0.207271  </td> <td>-0.0612626</td> <td>-0.0199006 </td> <td>-0.0398958</td> <td>0.117036   </td> <td>0.192392  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0669356</td> <td>-0.0167555 </td> <td>0.456054   </td> <td>0.465616   </td> <td>0.49979   </td> <td>0.445814   </td> <td>1         </td> <td>-0.504178  </td> <td>-0.271139  </td> <td>0.0776373  </td> <td>-0.167427  </td> <td>0.358701  </td> <td>-0.343335  </td> <td>-0.283579  </td> <td>-0.113535  </td> <td>0.101167   </td> <td>0.0547469  </td> <td>-0.347079 </td> <td>-0.0496912 </td> <td>-0.0590041 </td> <td>-0.0600881 </td> <td>0.0990467 </td> <td>-0.0848638 </td> <td>-0.172171  </td> <td>0.0231741 </td> <td>0.0464547  </td> <td>0.0575197 </td> <td>0.0288741  </td> <td>0.118706  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0993985 </td> <td>0.0508049  </td> <td>-0.252286  </td> <td>0.121964   </td> <td>-0.0134172</td> <td>-0.112952  </td> <td>-0.504178 </td> <td>1          </td> <td>0.0909023  </td> <td>0.11148    </td> <td>0.0368351  </td> <td>0.0820644 </td> <td>-0.0710807 </td> <td>0.128199   </td> <td>0.101809   </td> <td>0.0670063  </td> <td>0.023643   </td> <td>-0.13213  </td> <td>0.0970196  </td> <td>-0.0404261 </td> <td>-0.0581417 </td> <td>-0.0785573</td> <td>0.0929878  </td> <td>0.025168   </td> <td>-0.0418563</td> <td>0.105211   </td> <td>-0.117033 </td> <td>-0.00103725</td> <td>-0.101873 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0403303 </td> <td>0.0191882  </td> <td>-0.104632  </td> <td>-0.150409  </td> <td>-0.250767 </td> <td>0.0722512  </td> <td>-0.271139 </td> <td>0.0909023  </td> <td>1          </td> <td>0.0671714  </td> <td>0.000227605</td> <td>-0.0145695</td> <td>0.27723    </td> <td>0.0296249  </td> <td>-0.0272623 </td> <td>-0.00313241</td> <td>0.0183326  </td> <td>0.200124  </td> <td>0.0150032  </td> <td>0.0119981  </td> <td>-0.0119299 </td> <td>-0.21494  </td> <td>0.134428   </td> <td>-0.0432394 </td> <td>-0.0802966</td> <td>-0.00783063</td> <td>-0.0566211</td> <td>0.0547967  </td> <td>-0.0141027</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0809352 </td> <td>0.0419085  </td> <td>0.0713877  </td> <td>0.0806306  </td> <td>0.0403531 </td> <td>0.0828252  </td> <td>0.0776373 </td> <td>0.11148    </td> <td>0.0671714  </td> <td>1          </td> <td>-0.0115292 </td> <td>0.100324  </td> <td>-0.0115292 </td> <td>-0.0153659 </td> <td>-0.0040414 </td> <td>-0.0141144 </td> <td>-0.0141905 </td> <td>-0.0243858</td> <td>-0.00572235</td> <td>-0.0040414 </td> <td>-0.0135695 </td> <td>-0.056223 </td> <td>-0.00680581</td> <td>-0.0324893 </td> <td>-0.0364188</td> <td>-0.0188099 </td> <td>-0.0283265</td> <td>0.150679   </td> <td>-0.0243858</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0780156 </td> <td>0.0146118  </td> <td>-0.163959  </td> <td>-0.209151  </td> <td>-0.280427 </td> <td>-0.154585  </td> <td>-0.167427 </td> <td>0.0368351  </td> <td>0.000227605</td> <td>-0.0115292 </td> <td>1          </td> <td>-0.108375 </td> <td>-0.0197531 </td> <td>-0.0263266 </td> <td>-0.00692419</td> <td>-0.0349099 </td> <td>-0.0243129 </td> <td>-0.0497581</td> <td>-0.00980419</td> <td>-0.00692419</td> <td>-0.0232488 </td> <td>-0.0963277</td> <td>-0.0182756 </td> <td>-0.00434878</td> <td>-0.0150257</td> <td>0.0269862  </td> <td>0.0653531 </td> <td>-0.0545089 </td> <td>0.00608454</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0350897</td> <td>0.0262867  </td> <td>0.270168   </td> <td>0.501085   </td> <td>0.298465  </td> <td>0.406754   </td> <td>0.358701  </td> <td>0.0820644  </td> <td>-0.0145695 </td> <td>0.100324   </td> <td>-0.108375  </td> <td>1         </td> <td>-0.108375  </td> <td>-0.14444   </td> <td>-0.0379894 </td> <td>-0.191532  </td> <td>-0.133392  </td> <td>-0.272996 </td> <td>-0.0537904 </td> <td>-0.0379894 </td> <td>-0.127554  </td> <td>-0.528499 </td> <td>-0.116766  </td> <td>-0.159146  </td> <td>-0.0183069</td> <td>0.0889912  </td> <td>0.0583155 </td> <td>0.072248   </td> <td>0.0930668 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.00854172</td> <td>0.0739206  </td> <td>-0.166887  </td> <td>-0.18765   </td> <td>-0.110946 </td> <td>-0.204334  </td> <td>-0.343335 </td> <td>-0.0710807 </td> <td>0.27723    </td> <td>-0.0115292 </td> <td>-0.0197531 </td> <td>-0.108375 </td> <td>1          </td> <td>-0.0263266 </td> <td>-0.00692419</td> <td>-0.0349099 </td> <td>-0.0243129 </td> <td>-0.0497581</td> <td>-0.00980419</td> <td>-0.00692419</td> <td>-0.0232488 </td> <td>-0.0963277</td> <td>-0.0182756 </td> <td>0.149598   </td> <td>-0.0623968</td> <td>-0.0651238 </td> <td>0.00841042</td> <td>-0.00239738</td> <td>0.00608454</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0363699 </td> <td>-0.0511023 </td> <td>-0.335407  </td> <td>-0.296502  </td> <td>-0.364331 </td> <td>-0.271685  </td> <td>-0.283579 </td> <td>0.128199   </td> <td>0.0296249  </td> <td>-0.0153659 </td> <td>-0.0263266 </td> <td>-0.14444  </td> <td>-0.0263266 </td> <td>1          </td> <td>-0.00922845</td> <td>-0.0465274 </td> <td>-0.0324038 </td> <td>-0.0663168</td> <td>-0.0130669 </td> <td>-0.00922845</td> <td>-0.0309857 </td> <td>-0.128384 </td> <td>0.054991   </td> <td>0.00397439 </td> <td>0.0250708 </td> <td>0.0184293  </td> <td>-0.0646831</td> <td>0.00672672 </td> <td>-0.0663168</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0538299</td> <td>-0.0104705 </td> <td>-0.0886557 </td> <td>-0.0827246 </td> <td>-0.106655 </td> <td>-0.0734151 </td> <td>-0.113535 </td> <td>0.101809   </td> <td>-0.0272623 </td> <td>-0.0040414 </td> <td>-0.00692419</td> <td>-0.0379894</td> <td>-0.00692419</td> <td>-0.00922845</td> <td>1          </td> <td>-0.0122372 </td> <td>-0.00852256</td> <td>-0.017442 </td> <td>-0.00343673</td> <td>-0.00242718</td> <td>-0.00814957</td> <td>-0.0337664</td> <td>-0.0226381 </td> <td>-0.0195124 </td> <td>0.11097   </td> <td>-0.0228283 </td> <td>-0.0170124</td> <td>-0.0191074 </td> <td>-0.017442 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0514758 </td> <td>0.0607627  </td> <td>0.0162477  </td> <td>0.0186327  </td> <td>-0.0929711</td> <td>0.030646   </td> <td>0.101167  </td> <td>0.0670063  </td> <td>-0.00313241</td> <td>-0.0141144 </td> <td>-0.0349099 </td> <td>-0.191532 </td> <td>-0.0349099 </td> <td>-0.0465274 </td> <td>-0.0122372 </td> <td>1          </td> <td>-0.0429684 </td> <td>-0.087938 </td> <td>-0.0173271 </td> <td>-0.0122372 </td> <td>-0.0410879 </td> <td>-0.170241 </td> <td>-0.0322987 </td> <td>-0.0379159 </td> <td>0.0292579 </td> <td>-0.00656929</td> <td>-0.0522265</td> <td>0.0878604  </td> <td>0.0107533 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.108693  </td> <td>0.0970676  </td> <td>-0.0760959 </td> <td>-0.0557266 </td> <td>-0.169593 </td> <td>-0.00203346</td> <td>0.0547469 </td> <td>0.023643   </td> <td>0.0183326  </td> <td>-0.0141905 </td> <td>-0.0243129 </td> <td>-0.133392 </td> <td>-0.0243129 </td> <td>-0.0324038 </td> <td>-0.00852256</td> <td>-0.0429684 </td> <td>1          </td> <td>-0.0612441</td> <td>-0.0120674 </td> <td>-0.00852256</td> <td>-0.0286155 </td> <td>-0.118564 </td> <td>0.0345005  </td> <td>0.0157011  </td> <td>-0.0379296</td> <td>-0.00457516</td> <td>0.0337143 </td> <td>-0.0670916 </td> <td>0.0304002 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.00545336</td> <td>0.0481744  </td> <td>-0.0995258 </td> <td>-0.424646  </td> <td>-0.669985 </td> <td>-0.166378  </td> <td>-0.347079 </td> <td>-0.13213   </td> <td>0.200124   </td> <td>-0.0243858 </td> <td>-0.0497581 </td> <td>-0.272996 </td> <td>-0.0497581 </td> <td>-0.0663168 </td> <td>-0.017442  </td> <td>-0.087938  </td> <td>-0.0612441 </td> <td>1         </td> <td>-0.0246968 </td> <td>-0.017442  </td> <td>-0.0585638 </td> <td>-0.24265  </td> <td>0.0604648  </td> <td>0.0845884  </td> <td>0.0295964 </td> <td>-0.0631663 </td> <td>-0.0474153</td> <td>-0.0459904 </td> <td>-0.0274849</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0157337 </td> <td>-0.0148255 </td> <td>-0.0492187 </td> <td>-0.0923705 </td> <td>-0.1273   </td> <td>-0.0752487 </td> <td>-0.0496912</td> <td>0.0970196  </td> <td>0.0150032  </td> <td>-0.00572235</td> <td>-0.00980419</td> <td>-0.0537904</td> <td>-0.00980419</td> <td>-0.0130669 </td> <td>-0.00343673</td> <td>-0.0173271 </td> <td>-0.0120674 </td> <td>-0.0246968</td> <td>1          </td> <td>-0.00343673</td> <td>-0.0115392 </td> <td>-0.047811 </td> <td>0.0598788  </td> <td>-0.0276283 </td> <td>0.0630783 </td> <td>-0.0323233 </td> <td>-0.0240884</td> <td>-0.0270548 </td> <td>-0.0246968</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0327592 </td> <td>-0.0520503 </td> <td>-0.115347  </td> <td>-0.0832568 </td> <td>-0.087337 </td> <td>-0.0948784 </td> <td>-0.0590041</td> <td>-0.0404261 </td> <td>0.0119981  </td> <td>-0.0040414 </td> <td>-0.00692419</td> <td>-0.0379894</td> <td>-0.00692419</td> <td>-0.00922845</td> <td>-0.00242718</td> <td>-0.0122372 </td> <td>-0.00852256</td> <td>-0.017442 </td> <td>-0.00343673</td> <td>1          </td> <td>-0.00814957</td> <td>-0.0337664</td> <td>-0.0226381 </td> <td>-0.0195124 </td> <td>-0.0218724</td> <td>-0.0228283 </td> <td>-0.0170124</td> <td>0.127029   </td> <td>-0.017442 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0901702 </td> <td>-0.0351559 </td> <td>-0.00249694</td> <td>-0.178424  </td> <td>0.0913372 </td> <td>-0.0586931 </td> <td>-0.0600881</td> <td>-0.0581417 </td> <td>-0.0119299 </td> <td>-0.0135695 </td> <td>-0.0232488 </td> <td>-0.127554 </td> <td>-0.0232488 </td> <td>-0.0309857 </td> <td>-0.00814957</td> <td>-0.0410879 </td> <td>-0.0286155 </td> <td>-0.0585638</td> <td>-0.0115392 </td> <td>-0.00814957</td> <td>1          </td> <td>-0.113375 </td> <td>-0.0363736 </td> <td>0.0662599  </td> <td>0.0482068 </td> <td>0.00219542 </td> <td>-0.0571211</td> <td>-0.019549  </td> <td>-0.0107637</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.103218 </td> <td>-0.110605  </td> <td>0.0638747  </td> <td>0.10278    </td> <td>0.51514   </td> <td>-0.0634771 </td> <td>0.0990467 </td> <td>-0.0785573 </td> <td>-0.21494   </td> <td>-0.056223  </td> <td>-0.0963277 </td> <td>-0.528499 </td> <td>-0.0963277 </td> <td>-0.128384  </td> <td>-0.0337664 </td> <td>-0.170241  </td> <td>-0.118564  </td> <td>-0.24265  </td> <td>-0.047811  </td> <td>-0.0337664 </td> <td>-0.113375  </td> <td>1         </td> <td>0.0819484  </td> <td>0.0622078  </td> <td>-0.0242722</td> <td>-0.0317384 </td> <td>0.01577   </td> <td>-0.050195  </td> <td>-0.061102 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0505909</td> <td>-0.0761121 </td> <td>0.00252989 </td> <td>-0.0466028 </td> <td>-0.0419475</td> <td>0.0358406  </td> <td>-0.0848638</td> <td>0.0929878  </td> <td>0.134428   </td> <td>-0.00680581</td> <td>-0.0182756 </td> <td>-0.116766 </td> <td>-0.0182756 </td> <td>0.054991   </td> <td>-0.0226381 </td> <td>-0.0322987 </td> <td>0.0345005  </td> <td>0.0604648 </td> <td>0.0598788  </td> <td>-0.0226381 </td> <td>-0.0363736 </td> <td>0.0819484 </td> <td>1          </td> <td>-0.181991  </td> <td>-0.204002 </td> <td>-0.212917  </td> <td>-0.158673 </td> <td>-0.178213  </td> <td>-0.16268  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.036501  </td> <td>0.0173001  </td> <td>-0.107021  </td> <td>-0.147485  </td> <td>-0.0694226</td> <td>-0.207271  </td> <td>-0.172171 </td> <td>0.025168   </td> <td>-0.0432394 </td> <td>-0.0324893 </td> <td>-0.00434878</td> <td>-0.159146 </td> <td>0.149598   </td> <td>0.00397439 </td> <td>-0.0195124 </td> <td>-0.0379159 </td> <td>0.0157011  </td> <td>0.0845884 </td> <td>-0.0276283 </td> <td>-0.0195124 </td> <td>0.0662599  </td> <td>0.0622078 </td> <td>-0.181991  </td> <td>1          </td> <td>-0.175835 </td> <td>-0.183519  </td> <td>-0.136764 </td> <td>-0.153606  </td> <td>-0.140219 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0169414 </td> <td>0.01034    </td> <td>-0.0516326 </td> <td>-0.0115224 </td> <td>-0.0340239</td> <td>-0.0612626 </td> <td>0.0231741 </td> <td>-0.0418563 </td> <td>-0.0802966 </td> <td>-0.0364188 </td> <td>-0.0150257 </td> <td>-0.0183069</td> <td>-0.0623968 </td> <td>0.0250708  </td> <td>0.11097    </td> <td>0.0292579  </td> <td>-0.0379296 </td> <td>0.0295964 </td> <td>0.0630783  </td> <td>-0.0218724 </td> <td>0.0482068  </td> <td>-0.0242722</td> <td>-0.204002  </td> <td>-0.175835  </td> <td>1         </td> <td>-0.205715  </td> <td>-0.153306 </td> <td>-0.172185  </td> <td>-0.157178 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0600415</td> <td>-0.028835  </td> <td>-0.0120804 </td> <td>0.0759467  </td> <td>0.0398968 </td> <td>-0.0199006 </td> <td>0.0464547 </td> <td>0.105211   </td> <td>-0.00783063</td> <td>-0.0188099 </td> <td>0.0269862  </td> <td>0.0889912 </td> <td>-0.0651238 </td> <td>0.0184293  </td> <td>-0.0228283 </td> <td>-0.00656929</td> <td>-0.00457516</td> <td>-0.0631663</td> <td>-0.0323233 </td> <td>-0.0228283 </td> <td>0.00219542 </td> <td>-0.0317384</td> <td>-0.212917  </td> <td>-0.183519  </td> <td>-0.205715 </td> <td>1          </td> <td>-0.160006 </td> <td>-0.17971   </td> <td>-0.164047 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>-0.0117728</td> <td>0.0193415  </td> <td>0.0588916  </td> <td>0.119571   </td> <td>0.0765573 </td> <td>-0.0398958 </td> <td>0.0575197 </td> <td>-0.117033  </td> <td>-0.0566211 </td> <td>-0.0283265 </td> <td>0.0653531  </td> <td>0.0583155 </td> <td>0.00841042 </td> <td>-0.0646831 </td> <td>-0.0170124 </td> <td>-0.0522265 </td> <td>0.0337143  </td> <td>-0.0474153</td> <td>-0.0240884 </td> <td>-0.0170124 </td> <td>-0.0571211 </td> <td>0.01577   </td> <td>-0.158673  </td> <td>-0.136764  </td> <td>-0.153306 </td> <td>-0.160006  </td> <td>1         </td> <td>-0.133925  </td> <td>-0.122253 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0148926 </td> <td>-0.00362522</td> <td>0.00471689 </td> <td>-0.00239231</td> <td>0.0145859 </td> <td>0.117036   </td> <td>0.0288741 </td> <td>-0.00103725</td> <td>0.0547967  </td> <td>0.150679   </td> <td>-0.0545089 </td> <td>0.072248  </td> <td>-0.00239738</td> <td>0.00672672 </td> <td>-0.0191074 </td> <td>0.0878604  </td> <td>-0.0670916 </td> <td>-0.0459904</td> <td>-0.0270548 </td> <td>0.127029   </td> <td>-0.019549  </td> <td>-0.050195 </td> <td>-0.178213  </td> <td>-0.153606  </td> <td>-0.172185 </td> <td>-0.17971   </td> <td>-0.133925 </td> <td>1          </td> <td>-0.137308 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>0.0697063 </td> <td>0.0806525  </td> <td>0.126117   </td> <td>0.0234951  </td> <td>0.0271583 </td> <td>0.192392   </td> <td>0.118706  </td> <td>-0.101873  </td> <td>-0.0141027 </td> <td>-0.0243858 </td> <td>0.00608454 </td> <td>0.0930668 </td> <td>0.00608454 </td> <td>-0.0663168 </td> <td>-0.017442  </td> <td>0.0107533  </td> <td>0.0304002  </td> <td>-0.0274849</td> <td>-0.0246968 </td> <td>-0.017442  </td> <td>-0.0107637 </td> <td>-0.061102 </td> <td>-0.16268   </td> <td>-0.140219  </td> <td>-0.157178 </td> <td>-0.164047  </td> <td>-0.122253 </td> <td>-0.137308  </td> <td>1         </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (5 Points)** Calculate Pearson correlation of each feature with the forest fire area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Labels</th> <th>Pearsonr</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>X     </td> <td>0.0633853  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Y     </td> <td>0.0448732  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>FFMC  </td> <td>0.040122   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>DMC   </td> <td>0.0729943  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>DC    </td> <td>0.0493832  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>ISI   </td> <td>0.00825769 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>temp  </td> <td>0.0978441  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>RH    </td> <td>-0.0755186 </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>wind  </td> <td>0.0123173  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>rain  </td> <td>-0.00736573</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (19 rows omitted)</p>"
      ],
      "text/plain": [
       "Labels | Pearsonr\n",
       "X      | 0.0633853\n",
       "Y      | 0.0448732\n",
       "FFMC   | 0.040122\n",
       "DMC    | 0.0729943\n",
       "DC     | 0.0493832\n",
       "ISI    | 0.00825769\n",
       "temp   | 0.0978441\n",
       "RH     | -0.0755186\n",
       "wind   | 0.0123173\n",
       "rain   | -0.00736573\n",
       "... (19 rows omitted)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = data2.drop('area').labels\n",
    "r_values=[]\n",
    "for i in np.arange(len(labels)):\n",
    "    r_values.append(pearsonr(data2.column('area'),data2.column(labels[i] ))[0])\n",
    "    \n",
    "corr_table=Table().with_columns('Labels',labels,'Pearsonr',r_values) \n",
    "corr_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (15 Points)Feature Selection** We want to use 5 features which have the highest orrelation with the forest fire area. However, we do not want to use any correlated feature such that no feature pair in these selected features should have a correlation higher than 0.5 or less than -0.5. If there is such a pair, remove the  feature which is less correlated with forest fire area and pick the feature which has the most correlation with the fire area among all other previosuly not selected features. Eventually you should have 5 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Labels</th> <th>Pearsonr</th> <th>abs</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>temp  </td> <td>0.0978441 </td> <td>0.0978441</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>sat   </td> <td>0.0878676 </td> <td>0.0878676</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>RH    </td> <td>-0.0755186</td> <td>0.0755186</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>DMC   </td> <td>0.0729943 </td> <td>0.0729943</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>X     </td> <td>0.0633853 </td> <td>0.0633853</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>sep   </td> <td>0.0565729 </td> <td>0.0565729</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Labels | Pearsonr   | abs\n",
       "temp   | 0.0978441  | 0.0978441\n",
       "sat    | 0.0878676  | 0.0878676\n",
       "RH     | -0.0755186 | 0.0755186\n",
       "DMC    | 0.0729943  | 0.0729943\n",
       "X      | 0.0633853  | 0.0633853\n",
       "sep    | 0.0565729  | 0.0565729"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_first5=corr_table.with_columns('abs',abs(corr_table.column('Pearsonr'))).sort('abs',descending=True).take(np.arange(6))\n",
    "corr_first5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlation(column1,column2):\n",
    "    return matrix_table[matrix_table.column_index(column1)][matrix_table.column_index(column2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp, temp, 1.0\n",
      "temp, sat, 0.023174109691778983\n",
      "temp, RH, -0.5041782189597932\n",
      "temp, DMC, 0.4656158030272235\n",
      "temp, X, -0.06693564998504759\n",
      "temp, sep, 0.09904670950618465\n",
      "sat, temp, 0.023174109691778983\n",
      "sat, sat, 1.0\n",
      "sat, RH, -0.04185629794255413\n",
      "sat, DMC, -0.011522378934034651\n",
      "sat, X, 0.016941405404874908\n",
      "sat, sep, -0.02427218607648703\n",
      "RH, temp, -0.5041782189597932\n",
      "RH, sat, -0.04185629794255413\n",
      "RH, RH, 1.0\n",
      "RH, DMC, 0.12196375031833154\n",
      "RH, X, 0.09939850480789052\n",
      "RH, sep, -0.07855725106077827\n",
      "DMC, temp, 0.4656158030272235\n",
      "DMC, sat, -0.011522378934034651\n",
      "DMC, RH, 0.12196375031833154\n",
      "DMC, DMC, 1.0\n",
      "DMC, X, -0.010330639455733061\n",
      "DMC, sep, 0.10278033298783745\n",
      "X, temp, -0.06693564998504759\n",
      "X, sat, 0.016941405404874908\n",
      "X, RH, 0.09939850480789052\n",
      "X, DMC, -0.010330639455733061\n",
      "X, X, 1.0\n",
      "X, sep, -0.10321780407952001\n",
      "sep, temp, 0.09904670950618465\n",
      "sep, sat, -0.02427218607648703\n",
      "sep, RH, -0.07855725106077827\n",
      "sep, DMC, 0.10278033298783745\n",
      "sep, X, -0.10321780407952001\n",
      "sep, sep, 1.0\n"
     ]
    }
   ],
   "source": [
    "for column1 in corr_first5.column(0):\n",
    "    for column2 in corr_first5.column(0):\n",
    "        print (column1 + \", \"+ column2 +\", \" + str(find_correlation(column1,column2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RH,temps values is lower than -0.5 so it is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Labels</th> <th>Pearsonr</th> <th>abs</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>temp  </td> <td>0.0978441</td> <td>0.0978441</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>sat   </td> <td>0.0878676</td> <td>0.0878676</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>DMC   </td> <td>0.0729943</td> <td>0.0729943</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>X     </td> <td>0.0633853</td> <td>0.0633853</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>sep   </td> <td>0.0565729</td> <td>0.0565729</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Labels | Pearsonr  | abs\n",
       "temp   | 0.0978441 | 0.0978441\n",
       "sat    | 0.0878676 | 0.0878676\n",
       "DMC    | 0.0729943 | 0.0729943\n",
       "X      | 0.0633853 | 0.0633853\n",
       "sep    | 0.0565729 | 0.0565729"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_first5=corr_first5.remove(2)\n",
    "corr_first5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (15 Points)** Build a linear regression model using the features selected in Question 5. Print the weigths of each feature and bias value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>temp</th> <th>sat</th> <th>DMC</th> <th>X</th> <th>sep</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>22.7</td> <td>0   </td> <td>108.4</td> <td>8   </td> <td>1   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>17.8</td> <td>1   </td> <td>43.7 </td> <td>7   </td> <td>0   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14.2</td> <td>1   </td> <td>253.6</td> <td>8   </td> <td>0   </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (410 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features_5=train.drop('area').select(corr_first5.column(0))\n",
    "train_features_5.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_train_with5(weights):\n",
    "    return rmse(weights[0:len(weights)-1],weights[len(weights)-1], train_features_5, train_areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best slopes for the training set:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>temp</th> <th>sat</th> <th>DMC</th> <th>X</th> <th>sep</th> <th>bias</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0.935529</td> <td>19.5177</td> <td>0.0379566</td> <td>2.61656</td> <td>9.6117</td> <td>-26.5263</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of all training examples using the best slopes: 68.6484051543383\n"
     ]
    }
   ],
   "source": [
    "initial_weights_with5 = np.ones( len(train_features_5.row(0) )+1)\n",
    "learned_weights_with5 = minimize(rmse_train_with5, start=initial_weights_with5, smooth=True, array=True)\n",
    "print('The best slopes for the training set:')\n",
    "features_names_with5 = np.append(train_features_5.labels,'bias')\n",
    "Table(features_names_with5).with_row(list(learned_weights_with5)).show()\n",
    "print('RMSE of all training examples using the best slopes:', rmse_train_with5(learned_weights_with5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7 (10 Points)**  Calculate root mean square error value of your models in Question 2 and Question 6 on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_area = test.column('area')\n",
    "test_features_Q2 = test.drop('area')\n",
    "test_features_Q6=test.select(corr_first5.column(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_test(weights,feature):\n",
    "    return rmse(weights[0:len(weights)-1],weights[len(weights)-1], feature, test_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score of Q2 Model: 32.41440979591175\n",
      "RMSE score of Q6 Model: 30.355589615542293\n"
     ]
    }
   ],
   "source": [
    "print('RMSE score of Q2 Model: '+ str(rmse_test(learned_weights,test_features_Q2)))\n",
    "print('RMSE score of Q6 Model: '+ str(rmse_test(learned_weights_with5,test_features_Q6)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8 (5 Points)** Plot the test data where x axis is the actual forest fire area and y axis is the predicted values by the model in Question 2 and Question 6. The colors for each model should be different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_Q2 = learned_weights[len(learned_weights)-1]\n",
    "bias_Q6 = learned_weights_with5[len(learned_weights_with5)-1]\n",
    "def predict_Q2(row):\n",
    "    return sum(learned_weights[0:len(learned_weights)-1] * np.array(row)) + bias_Q2\n",
    "def predict_Q6(row):\n",
    "    return sum(learned_weights_with5[0:len(learned_weights_with5)-1] * np.array(row)) + bias_Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_prediction=test.with_columns('Prediction_Q2', test.drop('area').apply(predict_Q2),'Prediction_Q6',test.select(corr_first5.column(0)).apply(predict_Q6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2c3d0240128>]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with_prediction.select('area','Prediction_Q2','Prediction_Q6').scatter('area')\n",
    "plots.plot([0,9e1],[0,9e1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9(20 Points)K-fold**. Build a knn model for this problem where the predicted area is the mean of the closest 3 neighbors. Calculate RMSE score for this model on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(pt1, pt2):\n",
    "    \"\"\"The distance between two points, represented as arrays.\"\"\"\n",
    "    return np.sqrt(sum((pt1 - pt2) ** 2))\n",
    "def row_distance(row1, row2):\n",
    "    \"\"\"The distance between two rows of a table.\"\"\"\n",
    "    return distance(np.array(row1), np.array(row2))\n",
    "def distances(training, example, output):\n",
    "    \"\"\"Compute the distance from example for each row in training.\"\"\"\n",
    "    dists = []\n",
    "    attributes = training.drop(output)\n",
    "    for row in attributes.rows:\n",
    "        dists.append(row_distance(row, example))\n",
    "    return training.with_column('Distance', dists)\n",
    "def closest(training, example, k, output):\n",
    "    \"\"\"Return a table of the k closest neighbors to example.\"\"\"\n",
    "    return distances(training, example, output).sort('Distance').take(np.arange(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(example):\n",
    "    \"\"\"Return the majority class among the k nearest neighbors.\"\"\"\n",
    "    return np.average(closest(train, example, 3, 'area').column('area'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set RMSE for multiple linear regression:  32.41440979591175\n",
      "Test set RMSE for nearest neighbor regression: 38.81583225507146\n"
     ]
    }
   ],
   "source": [
    "prediction_with_knn = test.drop('area').apply(predict_knn)\n",
    "rmse_knn = np.mean((test_area - prediction_with_knn) ** 2) ** 0.5\n",
    "rmse_linear=rmse_test(learned_weights,test_features_Q2)\n",
    "print('Test set RMSE for multiple linear regression: ', rmse_linear)\n",
    "print('Test set RMSE for nearest neighbor regression:', rmse_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
